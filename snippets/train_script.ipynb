{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "f23cf0b9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Looking in indexes: https://download.pytorch.org/whl/cu124\n",
      "Requirement already satisfied: torch in c:\\users\\prasasnna\\miniconda3\\envs\\blackwell\\lib\\site-packages (2.12.0.dev20260221+cu128)\n",
      "Requirement already satisfied: torchvision in c:\\users\\prasasnna\\miniconda3\\envs\\blackwell\\lib\\site-packages (0.26.0.dev20260220+cu128)\n",
      "Requirement already satisfied: torchaudio in c:\\users\\prasasnna\\miniconda3\\envs\\blackwell\\lib\\site-packages (2.11.0.dev20260221+cu128)\n",
      "Requirement already satisfied: filelock in c:\\users\\prasasnna\\miniconda3\\envs\\blackwell\\lib\\site-packages (from torch) (3.20.3)\n",
      "Requirement already satisfied: typing-extensions>=4.10.0 in c:\\users\\prasasnna\\miniconda3\\envs\\blackwell\\lib\\site-packages (from torch) (4.15.0)\n",
      "Requirement already satisfied: sympy>=1.13.3 in c:\\users\\prasasnna\\miniconda3\\envs\\blackwell\\lib\\site-packages (from torch) (1.14.0)\n",
      "Requirement already satisfied: networkx>=2.5.1 in c:\\users\\prasasnna\\miniconda3\\envs\\blackwell\\lib\\site-packages (from torch) (3.4.2)\n",
      "Requirement already satisfied: jinja2 in c:\\users\\prasasnna\\miniconda3\\envs\\blackwell\\lib\\site-packages (from torch) (3.1.6)\n",
      "Requirement already satisfied: fsspec>=0.8.5 in c:\\users\\prasasnna\\miniconda3\\envs\\blackwell\\lib\\site-packages (from torch) (2025.9.0)\n",
      "Requirement already satisfied: numpy in c:\\users\\prasasnna\\miniconda3\\envs\\blackwell\\lib\\site-packages (from torchvision) (2.2.6)\n",
      "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in c:\\users\\prasasnna\\miniconda3\\envs\\blackwell\\lib\\site-packages (from torchvision) (12.1.0)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in c:\\users\\prasasnna\\miniconda3\\envs\\blackwell\\lib\\site-packages (from sympy>=1.13.3->torch) (1.3.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in c:\\users\\prasasnna\\miniconda3\\envs\\blackwell\\lib\\site-packages (from jinja2->torch) (3.0.2)\n",
      "Requirement already satisfied: datasets in c:\\users\\prasasnna\\miniconda3\\envs\\blackwell\\lib\\site-packages (4.3.0)\n",
      "Requirement already satisfied: filelock in c:\\users\\prasasnna\\miniconda3\\envs\\blackwell\\lib\\site-packages (from datasets) (3.20.3)\n",
      "Requirement already satisfied: numpy>=1.17 in c:\\users\\prasasnna\\miniconda3\\envs\\blackwell\\lib\\site-packages (from datasets) (2.2.6)\n",
      "Requirement already satisfied: pyarrow>=21.0.0 in c:\\users\\prasasnna\\miniconda3\\envs\\blackwell\\lib\\site-packages (from datasets) (23.0.1)\n",
      "Requirement already satisfied: dill<0.4.1,>=0.3.0 in c:\\users\\prasasnna\\miniconda3\\envs\\blackwell\\lib\\site-packages (from datasets) (0.4.0)\n",
      "Requirement already satisfied: pandas in c:\\users\\prasasnna\\miniconda3\\envs\\blackwell\\lib\\site-packages (from datasets) (2.3.3)\n",
      "Requirement already satisfied: requests>=2.32.2 in c:\\users\\prasasnna\\miniconda3\\envs\\blackwell\\lib\\site-packages (from datasets) (2.32.5)\n",
      "Requirement already satisfied: httpx<1.0.0 in c:\\users\\prasasnna\\miniconda3\\envs\\blackwell\\lib\\site-packages (from datasets) (0.28.1)\n",
      "Requirement already satisfied: tqdm>=4.66.3 in c:\\users\\prasasnna\\miniconda3\\envs\\blackwell\\lib\\site-packages (from datasets) (4.67.3)\n",
      "Requirement already satisfied: xxhash in c:\\users\\prasasnna\\miniconda3\\envs\\blackwell\\lib\\site-packages (from datasets) (3.6.0)\n",
      "Requirement already satisfied: multiprocess<0.70.17 in c:\\users\\prasasnna\\miniconda3\\envs\\blackwell\\lib\\site-packages (from datasets) (0.70.16)\n",
      "Requirement already satisfied: fsspec<=2025.9.0,>=2023.1.0 in c:\\users\\prasasnna\\miniconda3\\envs\\blackwell\\lib\\site-packages (from fsspec[http]<=2025.9.0,>=2023.1.0->datasets) (2025.9.0)\n",
      "Requirement already satisfied: huggingface-hub<2.0,>=0.25.0 in c:\\users\\prasasnna\\miniconda3\\envs\\blackwell\\lib\\site-packages (from datasets) (0.36.2)\n",
      "Requirement already satisfied: packaging in c:\\users\\prasasnna\\miniconda3\\envs\\blackwell\\lib\\site-packages (from datasets) (25.0)\n",
      "Requirement already satisfied: pyyaml>=5.1 in c:\\users\\prasasnna\\miniconda3\\envs\\blackwell\\lib\\site-packages (from datasets) (6.0.3)\n",
      "Requirement already satisfied: aiohttp!=4.0.0a0,!=4.0.0a1 in c:\\users\\prasasnna\\miniconda3\\envs\\blackwell\\lib\\site-packages (from fsspec[http]<=2025.9.0,>=2023.1.0->datasets) (3.13.3)\n",
      "Requirement already satisfied: anyio in c:\\users\\prasasnna\\miniconda3\\envs\\blackwell\\lib\\site-packages (from httpx<1.0.0->datasets) (4.12.1)\n",
      "Requirement already satisfied: certifi in c:\\users\\prasasnna\\miniconda3\\envs\\blackwell\\lib\\site-packages (from httpx<1.0.0->datasets) (2026.1.4)\n",
      "Requirement already satisfied: httpcore==1.* in c:\\users\\prasasnna\\miniconda3\\envs\\blackwell\\lib\\site-packages (from httpx<1.0.0->datasets) (1.0.9)\n",
      "Requirement already satisfied: idna in c:\\users\\prasasnna\\miniconda3\\envs\\blackwell\\lib\\site-packages (from httpx<1.0.0->datasets) (3.11)\n",
      "Requirement already satisfied: h11>=0.16 in c:\\users\\prasasnna\\miniconda3\\envs\\blackwell\\lib\\site-packages (from httpcore==1.*->httpx<1.0.0->datasets) (0.16.0)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in c:\\users\\prasasnna\\miniconda3\\envs\\blackwell\\lib\\site-packages (from huggingface-hub<2.0,>=0.25.0->datasets) (4.15.0)\n",
      "Requirement already satisfied: aiohappyeyeballs>=2.5.0 in c:\\users\\prasasnna\\miniconda3\\envs\\blackwell\\lib\\site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.9.0,>=2023.1.0->datasets) (2.6.1)\n",
      "Requirement already satisfied: aiosignal>=1.4.0 in c:\\users\\prasasnna\\miniconda3\\envs\\blackwell\\lib\\site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.9.0,>=2023.1.0->datasets) (1.4.0)\n",
      "Requirement already satisfied: async-timeout<6.0,>=4.0 in c:\\users\\prasasnna\\miniconda3\\envs\\blackwell\\lib\\site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.9.0,>=2023.1.0->datasets) (5.0.1)\n",
      "Requirement already satisfied: attrs>=17.3.0 in c:\\users\\prasasnna\\miniconda3\\envs\\blackwell\\lib\\site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.9.0,>=2023.1.0->datasets) (25.4.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in c:\\users\\prasasnna\\miniconda3\\envs\\blackwell\\lib\\site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.9.0,>=2023.1.0->datasets) (1.8.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in c:\\users\\prasasnna\\miniconda3\\envs\\blackwell\\lib\\site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.9.0,>=2023.1.0->datasets) (6.7.1)\n",
      "Requirement already satisfied: propcache>=0.2.0 in c:\\users\\prasasnna\\miniconda3\\envs\\blackwell\\lib\\site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.9.0,>=2023.1.0->datasets) (0.4.1)\n",
      "Requirement already satisfied: yarl<2.0,>=1.17.0 in c:\\users\\prasasnna\\miniconda3\\envs\\blackwell\\lib\\site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.9.0,>=2023.1.0->datasets) (1.22.0)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in c:\\users\\prasasnna\\miniconda3\\envs\\blackwell\\lib\\site-packages (from requests>=2.32.2->datasets) (3.4.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\prasasnna\\miniconda3\\envs\\blackwell\\lib\\site-packages (from requests>=2.32.2->datasets) (2.6.3)\n",
      "Requirement already satisfied: colorama in c:\\users\\prasasnna\\miniconda3\\envs\\blackwell\\lib\\site-packages (from tqdm>=4.66.3->datasets) (0.4.6)\n",
      "Requirement already satisfied: exceptiongroup>=1.0.2 in c:\\users\\prasasnna\\miniconda3\\envs\\blackwell\\lib\\site-packages (from anyio->httpx<1.0.0->datasets) (1.3.1)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in c:\\users\\prasasnna\\miniconda3\\envs\\blackwell\\lib\\site-packages (from pandas->datasets) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in c:\\users\\prasasnna\\miniconda3\\envs\\blackwell\\lib\\site-packages (from pandas->datasets) (2025.2)\n",
      "Requirement already satisfied: tzdata>=2022.7 in c:\\users\\prasasnna\\miniconda3\\envs\\blackwell\\lib\\site-packages (from pandas->datasets) (2025.3)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\prasasnna\\miniconda3\\envs\\blackwell\\lib\\site-packages (from python-dateutil>=2.8.2->pandas->datasets) (1.17.0)\n",
      "Requirement already satisfied: accelerate in c:\\users\\prasasnna\\miniconda3\\envs\\blackwell\\lib\\site-packages (1.12.0)\n",
      "Requirement already satisfied: numpy>=1.17 in c:\\users\\prasasnna\\miniconda3\\envs\\blackwell\\lib\\site-packages (from accelerate) (2.2.6)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\users\\prasasnna\\miniconda3\\envs\\blackwell\\lib\\site-packages (from accelerate) (25.0)\n",
      "Requirement already satisfied: psutil in c:\\users\\prasasnna\\miniconda3\\envs\\blackwell\\lib\\site-packages (from accelerate) (7.2.2)\n",
      "Requirement already satisfied: pyyaml in c:\\users\\prasasnna\\miniconda3\\envs\\blackwell\\lib\\site-packages (from accelerate) (6.0.3)\n",
      "Requirement already satisfied: torch>=2.0.0 in c:\\users\\prasasnna\\miniconda3\\envs\\blackwell\\lib\\site-packages (from accelerate) (2.12.0.dev20260221+cu128)\n",
      "Requirement already satisfied: huggingface_hub>=0.21.0 in c:\\users\\prasasnna\\miniconda3\\envs\\blackwell\\lib\\site-packages (from accelerate) (0.36.2)\n",
      "Requirement already satisfied: safetensors>=0.4.3 in c:\\users\\prasasnna\\miniconda3\\envs\\blackwell\\lib\\site-packages (from accelerate) (0.7.0)\n",
      "Requirement already satisfied: filelock in c:\\users\\prasasnna\\miniconda3\\envs\\blackwell\\lib\\site-packages (from huggingface_hub>=0.21.0->accelerate) (3.20.3)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in c:\\users\\prasasnna\\miniconda3\\envs\\blackwell\\lib\\site-packages (from huggingface_hub>=0.21.0->accelerate) (2025.9.0)\n",
      "Requirement already satisfied: requests in c:\\users\\prasasnna\\miniconda3\\envs\\blackwell\\lib\\site-packages (from huggingface_hub>=0.21.0->accelerate) (2.32.5)\n",
      "Requirement already satisfied: tqdm>=4.42.1 in c:\\users\\prasasnna\\miniconda3\\envs\\blackwell\\lib\\site-packages (from huggingface_hub>=0.21.0->accelerate) (4.67.3)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in c:\\users\\prasasnna\\miniconda3\\envs\\blackwell\\lib\\site-packages (from huggingface_hub>=0.21.0->accelerate) (4.15.0)\n",
      "Requirement already satisfied: sympy>=1.13.3 in c:\\users\\prasasnna\\miniconda3\\envs\\blackwell\\lib\\site-packages (from torch>=2.0.0->accelerate) (1.14.0)\n",
      "Requirement already satisfied: networkx>=2.5.1 in c:\\users\\prasasnna\\miniconda3\\envs\\blackwell\\lib\\site-packages (from torch>=2.0.0->accelerate) (3.4.2)\n",
      "Requirement already satisfied: jinja2 in c:\\users\\prasasnna\\miniconda3\\envs\\blackwell\\lib\\site-packages (from torch>=2.0.0->accelerate) (3.1.6)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in c:\\users\\prasasnna\\miniconda3\\envs\\blackwell\\lib\\site-packages (from sympy>=1.13.3->torch>=2.0.0->accelerate) (1.3.0)\n",
      "Requirement already satisfied: colorama in c:\\users\\prasasnna\\miniconda3\\envs\\blackwell\\lib\\site-packages (from tqdm>=4.42.1->huggingface_hub>=0.21.0->accelerate) (0.4.6)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in c:\\users\\prasasnna\\miniconda3\\envs\\blackwell\\lib\\site-packages (from jinja2->torch>=2.0.0->accelerate) (3.0.2)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in c:\\users\\prasasnna\\miniconda3\\envs\\blackwell\\lib\\site-packages (from requests->huggingface_hub>=0.21.0->accelerate) (3.4.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\prasasnna\\miniconda3\\envs\\blackwell\\lib\\site-packages (from requests->huggingface_hub>=0.21.0->accelerate) (3.11)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\prasasnna\\miniconda3\\envs\\blackwell\\lib\\site-packages (from requests->huggingface_hub>=0.21.0->accelerate) (2.6.3)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\prasasnna\\miniconda3\\envs\\blackwell\\lib\\site-packages (from requests->huggingface_hub>=0.21.0->accelerate) (2026.1.4)\n",
      "Requirement already satisfied: sentencepiece in c:\\users\\prasasnna\\miniconda3\\envs\\blackwell\\lib\\site-packages (0.2.1)\n",
      "Requirement already satisfied: safetensors in c:\\users\\prasasnna\\miniconda3\\envs\\blackwell\\lib\\site-packages (0.7.0)\n",
      "Collecting scipy\n",
      "  Downloading scipy-1.15.3-cp310-cp310-win_amd64.whl.metadata (60 kB)\n",
      "Requirement already satisfied: numpy<2.5,>=1.23.5 in c:\\users\\prasasnna\\miniconda3\\envs\\blackwell\\lib\\site-packages (from scipy) (2.2.6)\n",
      "Downloading scipy-1.15.3-cp310-cp310-win_amd64.whl (41.3 MB)\n",
      "   ---------------------------------------- 0.0/41.3 MB ? eta -:--:--\n",
      "   ---------------------------------------- 0.3/41.3 MB ? eta -:--:--\n",
      "    --------------------------------------- 0.8/41.3 MB 2.2 MB/s eta 0:00:19\n",
      "   - -------------------------------------- 1.6/41.3 MB 3.0 MB/s eta 0:00:14\n",
      "   -- ------------------------------------- 2.4/41.3 MB 3.4 MB/s eta 0:00:12\n",
      "   --- ------------------------------------ 3.1/41.3 MB 3.5 MB/s eta 0:00:11\n",
      "   ---- ----------------------------------- 4.2/41.3 MB 3.6 MB/s eta 0:00:11\n",
      "   ---- ----------------------------------- 4.7/41.3 MB 3.8 MB/s eta 0:00:10\n",
      "   ----- ---------------------------------- 5.5/41.3 MB 3.6 MB/s eta 0:00:10\n",
      "   ------ --------------------------------- 6.3/41.3 MB 3.7 MB/s eta 0:00:10\n",
      "   ------ --------------------------------- 7.1/41.3 MB 3.7 MB/s eta 0:00:10\n",
      "   ------- -------------------------------- 7.9/41.3 MB 3.7 MB/s eta 0:00:10\n",
      "   -------- ------------------------------- 8.9/41.3 MB 3.7 MB/s eta 0:00:09\n",
      "   --------- ------------------------------ 9.4/41.3 MB 3.8 MB/s eta 0:00:09\n",
      "   --------- ------------------------------ 10.2/41.3 MB 3.7 MB/s eta 0:00:09\n",
      "   ---------- ----------------------------- 11.0/41.3 MB 3.7 MB/s eta 0:00:09\n",
      "   ----------- ---------------------------- 11.8/41.3 MB 3.7 MB/s eta 0:00:08\n",
      "   ------------ --------------------------- 12.6/41.3 MB 3.7 MB/s eta 0:00:08\n",
      "   ------------ --------------------------- 13.4/41.3 MB 3.7 MB/s eta 0:00:08\n",
      "   ------------- -------------------------- 13.9/41.3 MB 3.7 MB/s eta 0:00:08\n",
      "   -------------- ------------------------- 14.7/41.3 MB 3.6 MB/s eta 0:00:08\n",
      "   -------------- ------------------------- 15.2/41.3 MB 3.6 MB/s eta 0:00:08\n",
      "   --------------- ------------------------ 16.0/41.3 MB 3.6 MB/s eta 0:00:08\n",
      "   ---------------- ----------------------- 16.5/41.3 MB 3.6 MB/s eta 0:00:07\n",
      "   ---------------- ----------------------- 17.3/41.3 MB 3.6 MB/s eta 0:00:07\n",
      "   ----------------- ---------------------- 17.8/41.3 MB 3.5 MB/s eta 0:00:07\n",
      "   ----------------- ---------------------- 18.4/41.3 MB 3.5 MB/s eta 0:00:07\n",
      "   ------------------ --------------------- 18.9/41.3 MB 3.5 MB/s eta 0:00:07\n",
      "   ------------------ --------------------- 19.4/41.3 MB 3.4 MB/s eta 0:00:07\n",
      "   ------------------- -------------------- 19.9/41.3 MB 3.4 MB/s eta 0:00:07\n",
      "   ------------------- -------------------- 20.4/41.3 MB 3.4 MB/s eta 0:00:07\n",
      "   -------------------- ------------------- 21.2/41.3 MB 3.4 MB/s eta 0:00:06\n",
      "   --------------------- ------------------ 21.8/41.3 MB 3.3 MB/s eta 0:00:06\n",
      "   --------------------- ------------------ 22.3/41.3 MB 3.3 MB/s eta 0:00:06\n",
      "   ---------------------- ----------------- 22.8/41.3 MB 3.3 MB/s eta 0:00:06\n",
      "   ---------------------- ----------------- 23.1/41.3 MB 3.3 MB/s eta 0:00:06\n",
      "   ---------------------- ----------------- 23.3/41.3 MB 3.3 MB/s eta 0:00:06\n",
      "   ----------------------- ---------------- 23.9/41.3 MB 3.2 MB/s eta 0:00:06\n",
      "   ----------------------- ---------------- 24.4/41.3 MB 3.2 MB/s eta 0:00:06\n",
      "   ------------------------ --------------- 24.9/41.3 MB 3.1 MB/s eta 0:00:06\n",
      "   ------------------------ --------------- 25.4/41.3 MB 3.1 MB/s eta 0:00:06\n",
      "   ------------------------- -------------- 26.2/41.3 MB 3.1 MB/s eta 0:00:05\n",
      "   ------------------------- -------------- 26.7/41.3 MB 3.1 MB/s eta 0:00:05\n",
      "   -------------------------- ------------- 27.5/41.3 MB 3.1 MB/s eta 0:00:05\n",
      "   --------------------------- ------------ 28.0/41.3 MB 3.2 MB/s eta 0:00:05\n",
      "   --------------------------- ------------ 28.8/41.3 MB 3.1 MB/s eta 0:00:04\n",
      "   ---------------------------- ----------- 29.4/41.3 MB 3.1 MB/s eta 0:00:04\n",
      "   ----------------------------- ---------- 30.1/41.3 MB 3.1 MB/s eta 0:00:04\n",
      "   ----------------------------- ---------- 30.7/41.3 MB 3.1 MB/s eta 0:00:04\n",
      "   ------------------------------ --------- 31.7/41.3 MB 3.2 MB/s eta 0:00:04\n",
      "   ------------------------------- -------- 32.5/41.3 MB 3.2 MB/s eta 0:00:03\n",
      "   -------------------------------- ------- 33.0/41.3 MB 3.2 MB/s eta 0:00:03\n",
      "   -------------------------------- ------- 33.8/41.3 MB 3.2 MB/s eta 0:00:03\n",
      "   --------------------------------- ------ 34.3/41.3 MB 3.2 MB/s eta 0:00:03\n",
      "   ---------------------------------- ----- 35.1/41.3 MB 3.2 MB/s eta 0:00:02\n",
      "   ---------------------------------- ----- 35.7/41.3 MB 3.2 MB/s eta 0:00:02\n",
      "   ----------------------------------- ---- 36.4/41.3 MB 3.2 MB/s eta 0:00:02\n",
      "   ----------------------------------- ---- 37.0/41.3 MB 3.2 MB/s eta 0:00:02\n",
      "   ------------------------------------ --- 37.7/41.3 MB 3.2 MB/s eta 0:00:02\n",
      "   ------------------------------------- -- 38.3/41.3 MB 3.2 MB/s eta 0:00:01\n",
      "   ------------------------------------- -- 38.8/41.3 MB 3.2 MB/s eta 0:00:01\n",
      "   -------------------------------------- - 39.6/41.3 MB 3.2 MB/s eta 0:00:01\n",
      "   ---------------------------------------  40.4/41.3 MB 3.2 MB/s eta 0:00:01\n",
      "   ---------------------------------------  40.9/41.3 MB 3.2 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 41.3/41.3 MB 3.2 MB/s  0:00:13\n",
      "Installing collected packages: scipy\n",
      "Successfully installed scipy-1.15.3\n"
     ]
    }
   ],
   "source": [
    "!pip install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu124\n",
    "!pip install transformers>=4.39.0\n",
    "!pip install datasets\n",
    "!pip install accelerate\n",
    "!pip install peft>=0.10.0\n",
    "!pip install trl>=0.8.6\n",
    "!pip install bitsandbytes>=0.43.0\n",
    "!pip install sentencepiece\n",
    "!pip install safetensors\n",
    "!pip install scipy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "30d50ce0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Torch: 2.12.0.dev20260221+cu128\n",
      "CUDA: 12.8\n",
      "Capability: (12, 0)\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "print(\"Torch:\", torch.__version__)\n",
    "print(\"CUDA:\", torch.version.cuda)\n",
    "print(\"Capability:\", torch.cuda.get_device_capability())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "00d166a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "# Disable ALL fused kernels\n",
    "os.environ[\"UNSLOTH_USE_XFORMERS\"] = \"0\"\n",
    "os.environ[\"UNSLOTH_FORCE_DISABLE_COMPILE\"] = \"1\"\n",
    "os.environ[\"UNSLOTH_DISABLE_FAST_GENERATION\"] = \"1\"\n",
    "os.environ[\"FLASH_ATTENTION_FORCE_DISABLE\"] = \"1\"\n",
    "os.environ[\"ACCELERATE_USE_SDPA\"] = \"true\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "cb3ebd10",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\prasasnna\\Miniconda3\\envs\\blackwell\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "Skipping import of cpp extensions due to incompatible torch version 2.12.0.dev20260221+cu128 for torchao version 0.16.0             Please see https://github.com/pytorch/ao/issues/2919 for more info\n",
      "W0221 20:31:22.091000 19848 site-packages\\torch\\distributed\\elastic\\multiprocessing\\redirects.py:29] NOTE: Redirects are currently not supported in Windows or MacOs.\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer, BitsAndBytesConfig\n",
    "from peft import LoraConfig, get_peft_model\n",
    "from trl import SFTTrainer, SFTConfig\n",
    "from datasets import load_dataset\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1bd21131",
   "metadata": {},
   "outputs": [],
   "source": [
    "BASE_DIR = os.getcwd()\n",
    "INPUT_PATH = os.path.join(BASE_DIR, \"..\", \"datasets\", \"L1_dataset.jsonl\")\n",
    "ADAPTER_PATH = os.path.join(BASE_DIR, \"..\", \"adapters\", \"aegis_L1\")\n",
    "\n",
    "model_name = \"Qwen/Qwen2.5-Coder-7B-Instruct\"\n",
    "max_seq_length = 2048"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "25e4c172",
   "metadata": {},
   "outputs": [],
   "source": [
    "bnb_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,\n",
    "    bnb_4bit_use_double_quant=True,\n",
    "    bnb_4bit_quant_type=\"nf4\",\n",
    "    bnb_4bit_compute_dtype=torch.bfloat16,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "15f5de22",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "tokenizer.pad_token = tokenizer.eos_token # Ensure padding matches EOS\n",
    "tokenizer.padding_side = \"right\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "868838c1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`torch_dtype` is deprecated! Use `dtype` instead!\n",
      "Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]c:\\Users\\prasasnna\\Miniconda3\\envs\\blackwell\\lib\\site-packages\\bitsandbytes\\backends\\cuda\\ops.py:213: FutureWarning: _check_is_size will be removed in a future PyTorch release along with guard_size_oblivious.     Use _check(i >= 0) instead.\n",
      "  torch._check_is_size(blocksize)\n",
      "Loading checkpoint shards: 100%|██████████| 4/4 [00:23<00:00,  5.76s/it]\n"
     ]
    }
   ],
   "source": [
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_name,\n",
    "    quantization_config=bnb_config,\n",
    "    torch_dtype=torch.bfloat16,\n",
    "    device_map=\"auto\",\n",
    "    attn_implementation=\"sdpa\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d65833e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "lora_config = LoraConfig(\n",
    "    r=64,\n",
    "    lora_alpha=128,\n",
    "    target_modules=[\"q_proj\",\"k_proj\",\"v_proj\",\"o_proj\", \"gate_proj\",\"up_proj\",\"down_proj\"],\n",
    "    lora_dropout=0,\n",
    "    bias=\"none\",\n",
    "    task_type=\"CAUSAL_LM\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "343cd9a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = get_peft_model(model, lora_config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "39674b8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = load_dataset(\"json\", data_files=INPUT_PATH, split=\"train\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "7767b6ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "system_prompt = \"You are the L1 Intent Layer for Mini Replit. Output strict JSON enums only.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "834b26ec",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 1001/1001 [00:00<00:00, 6128.27 examples/s]\n"
     ]
    }
   ],
   "source": [
    "def add_system_and_format(example):\n",
    "    messages = example[\"messages\"]\n",
    "    if messages[0][\"role\"] != \"system\":\n",
    "        messages.insert(0, {\"role\": \"system\", \"content\": system_prompt})\n",
    "    \n",
    "    # ✅ FIX 2: Apply chat template here to ensure the model learns the exact stop tokens\n",
    "    text = tokenizer.apply_chat_template(messages, tokenize=False, add_generation_prompt=False)\n",
    "    return {\"text\": text}\n",
    "\n",
    "dataset = dataset.map(add_system_and_format)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "6c410004",
   "metadata": {},
   "outputs": [],
   "source": [
    "sft_config = SFTConfig(\n",
    "    output_dir=\"outputs\",\n",
    "    max_length=max_seq_length,\n",
    "    dataset_text_field=\"text\", # ✅ FIX 3: Explicitly tell trainer to use our formatted string\n",
    "    per_device_train_batch_size=4,\n",
    "    gradient_accumulation_steps=4,\n",
    "    num_train_epochs=3,\n",
    "    learning_rate=2e-4,\n",
    "    bf16=True,\n",
    "    fp16=False,\n",
    "    optim=\"adamw_8bit\",\n",
    "    logging_steps=1,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "7791008a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Tokenizing train dataset: 100%|██████████| 1001/1001 [00:00<00:00, 1074.13 examples/s]\n",
      "Truncating train dataset: 100%|██████████| 1001/1001 [00:00<00:00, 112656.93 examples/s]\n"
     ]
    }
   ],
   "source": [
    "trainer = SFTTrainer(\n",
    "    model=model,\n",
    "    train_dataset=dataset,\n",
    "    args=sft_config,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "54b4c180",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The tokenizer has new PAD/BOS/EOS tokens that differ from the model config and generation config. The model config and generation config were aligned accordingly, being updated with the tokenizer's values. Updated tokens: {'bos_token_id': None, 'pad_token_id': 151643}.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='189' max='189' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [189/189 10:36, Epoch 3/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>13.343300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>12.856500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>11.571700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>15.744300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>12.384100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>11.124900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>10.261700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>10.324500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9</td>\n",
       "      <td>9.473800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>8.621700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11</td>\n",
       "      <td>7.906800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12</td>\n",
       "      <td>7.576600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>13</td>\n",
       "      <td>6.955200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>14</td>\n",
       "      <td>7.309800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>15</td>\n",
       "      <td>6.571600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>16</td>\n",
       "      <td>6.614000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>17</td>\n",
       "      <td>6.549900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>18</td>\n",
       "      <td>6.339100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>19</td>\n",
       "      <td>6.011100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>20</td>\n",
       "      <td>5.985200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>21</td>\n",
       "      <td>5.695000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>22</td>\n",
       "      <td>5.565900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>23</td>\n",
       "      <td>5.683800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>24</td>\n",
       "      <td>5.754500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>25</td>\n",
       "      <td>5.613200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>26</td>\n",
       "      <td>5.380600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>27</td>\n",
       "      <td>5.013000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>28</td>\n",
       "      <td>5.116600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>29</td>\n",
       "      <td>5.369200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>30</td>\n",
       "      <td>5.175700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>31</td>\n",
       "      <td>5.214300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>32</td>\n",
       "      <td>5.163000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>33</td>\n",
       "      <td>5.382600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>34</td>\n",
       "      <td>5.269400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>35</td>\n",
       "      <td>5.531100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>36</td>\n",
       "      <td>5.145300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>37</td>\n",
       "      <td>4.993300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>38</td>\n",
       "      <td>5.156400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>39</td>\n",
       "      <td>5.172500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>40</td>\n",
       "      <td>5.214700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>41</td>\n",
       "      <td>5.413500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>42</td>\n",
       "      <td>4.918100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>43</td>\n",
       "      <td>5.023900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>44</td>\n",
       "      <td>4.899000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>45</td>\n",
       "      <td>4.970100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>46</td>\n",
       "      <td>4.920500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>47</td>\n",
       "      <td>4.694700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>48</td>\n",
       "      <td>4.820900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>49</td>\n",
       "      <td>4.602800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>50</td>\n",
       "      <td>4.617700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>51</td>\n",
       "      <td>4.553100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>52</td>\n",
       "      <td>4.392300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>53</td>\n",
       "      <td>4.586300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>54</td>\n",
       "      <td>4.392600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>55</td>\n",
       "      <td>4.716400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>56</td>\n",
       "      <td>4.456600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>57</td>\n",
       "      <td>3.897800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>58</td>\n",
       "      <td>4.033500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>59</td>\n",
       "      <td>3.954600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>60</td>\n",
       "      <td>3.895100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>61</td>\n",
       "      <td>4.377300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>62</td>\n",
       "      <td>4.326100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>63</td>\n",
       "      <td>4.221400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>64</td>\n",
       "      <td>3.481100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>65</td>\n",
       "      <td>3.751800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>66</td>\n",
       "      <td>3.642200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>67</td>\n",
       "      <td>4.116300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>68</td>\n",
       "      <td>3.682000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>69</td>\n",
       "      <td>3.771200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>70</td>\n",
       "      <td>3.968000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>71</td>\n",
       "      <td>4.685500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>72</td>\n",
       "      <td>3.979800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>73</td>\n",
       "      <td>3.513900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>74</td>\n",
       "      <td>3.340900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>75</td>\n",
       "      <td>3.056100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>76</td>\n",
       "      <td>3.257700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>77</td>\n",
       "      <td>3.180100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>78</td>\n",
       "      <td>3.301700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>79</td>\n",
       "      <td>2.885100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>80</td>\n",
       "      <td>3.042600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>81</td>\n",
       "      <td>2.961100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>82</td>\n",
       "      <td>2.899500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>83</td>\n",
       "      <td>2.394400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>84</td>\n",
       "      <td>2.809700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>85</td>\n",
       "      <td>2.860600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>86</td>\n",
       "      <td>2.864100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>87</td>\n",
       "      <td>3.128800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>88</td>\n",
       "      <td>2.659400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>89</td>\n",
       "      <td>2.418600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>90</td>\n",
       "      <td>2.636700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>91</td>\n",
       "      <td>2.370100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>92</td>\n",
       "      <td>2.577200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>93</td>\n",
       "      <td>2.361400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>94</td>\n",
       "      <td>2.508900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>95</td>\n",
       "      <td>2.136100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>96</td>\n",
       "      <td>2.251700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>97</td>\n",
       "      <td>2.263600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>98</td>\n",
       "      <td>2.394200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>99</td>\n",
       "      <td>2.127100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>100</td>\n",
       "      <td>2.007100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>101</td>\n",
       "      <td>1.820700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>102</td>\n",
       "      <td>2.035400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>103</td>\n",
       "      <td>2.421700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>104</td>\n",
       "      <td>2.019900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>105</td>\n",
       "      <td>1.930400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>106</td>\n",
       "      <td>1.923300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>107</td>\n",
       "      <td>1.691900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>108</td>\n",
       "      <td>1.814200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>109</td>\n",
       "      <td>1.727000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>110</td>\n",
       "      <td>1.661300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>111</td>\n",
       "      <td>1.794800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>112</td>\n",
       "      <td>1.490400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>113</td>\n",
       "      <td>1.827700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>114</td>\n",
       "      <td>1.942200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>115</td>\n",
       "      <td>1.792300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>116</td>\n",
       "      <td>1.696500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>117</td>\n",
       "      <td>1.462200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>118</td>\n",
       "      <td>1.641000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>119</td>\n",
       "      <td>1.682000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>120</td>\n",
       "      <td>1.612700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>121</td>\n",
       "      <td>1.808800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>122</td>\n",
       "      <td>1.936300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>123</td>\n",
       "      <td>1.805300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>124</td>\n",
       "      <td>1.666800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>125</td>\n",
       "      <td>1.507600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>126</td>\n",
       "      <td>1.443700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>127</td>\n",
       "      <td>1.513200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>128</td>\n",
       "      <td>1.471400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>129</td>\n",
       "      <td>1.656600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>130</td>\n",
       "      <td>1.570800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>131</td>\n",
       "      <td>1.680400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>132</td>\n",
       "      <td>1.429300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>133</td>\n",
       "      <td>1.432400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>134</td>\n",
       "      <td>1.312900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>135</td>\n",
       "      <td>1.476400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>136</td>\n",
       "      <td>1.562500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>137</td>\n",
       "      <td>1.575200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>138</td>\n",
       "      <td>1.488400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>139</td>\n",
       "      <td>1.596700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>140</td>\n",
       "      <td>1.396200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>141</td>\n",
       "      <td>1.418500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>142</td>\n",
       "      <td>1.379200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>143</td>\n",
       "      <td>1.337700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>144</td>\n",
       "      <td>1.355100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>145</td>\n",
       "      <td>1.261900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>146</td>\n",
       "      <td>1.511600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>147</td>\n",
       "      <td>1.462700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>148</td>\n",
       "      <td>1.284100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>149</td>\n",
       "      <td>0.957200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>150</td>\n",
       "      <td>1.464400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>151</td>\n",
       "      <td>1.313400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>152</td>\n",
       "      <td>1.282900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>153</td>\n",
       "      <td>1.415800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>154</td>\n",
       "      <td>1.231700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>155</td>\n",
       "      <td>1.364000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>156</td>\n",
       "      <td>1.375600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>157</td>\n",
       "      <td>1.206800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>158</td>\n",
       "      <td>1.555700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>159</td>\n",
       "      <td>0.860300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>160</td>\n",
       "      <td>1.171900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>161</td>\n",
       "      <td>1.277300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>162</td>\n",
       "      <td>1.252100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>163</td>\n",
       "      <td>1.080000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>164</td>\n",
       "      <td>1.047500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>165</td>\n",
       "      <td>1.277800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>166</td>\n",
       "      <td>1.375800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>167</td>\n",
       "      <td>1.224500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>168</td>\n",
       "      <td>1.171100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>169</td>\n",
       "      <td>1.246400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>170</td>\n",
       "      <td>1.197600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>171</td>\n",
       "      <td>1.225200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>172</td>\n",
       "      <td>1.240800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>173</td>\n",
       "      <td>1.422500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>174</td>\n",
       "      <td>1.139000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>175</td>\n",
       "      <td>1.197300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>176</td>\n",
       "      <td>1.375000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>177</td>\n",
       "      <td>1.389200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>178</td>\n",
       "      <td>1.258100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>179</td>\n",
       "      <td>1.300300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>180</td>\n",
       "      <td>1.000200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>181</td>\n",
       "      <td>0.964900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>182</td>\n",
       "      <td>1.180400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>183</td>\n",
       "      <td>1.126600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>184</td>\n",
       "      <td>0.922300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>185</td>\n",
       "      <td>1.409300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>186</td>\n",
       "      <td>1.053800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>187</td>\n",
       "      <td>1.106700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>188</td>\n",
       "      <td>1.114100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>189</td>\n",
       "      <td>1.095700</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=189, training_loss=3.3561217677656305, metrics={'train_runtime': 640.7059, 'train_samples_per_second': 4.687, 'train_steps_per_second': 0.295, 'total_flos': 1.48339769448192e+16, 'train_loss': 3.3561217677656305, 'epoch': 3.0})"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "bdfe06b2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Adapter saved to d:\\Python\\AegisFlow-\\snippets\\..\\adapters\\aegis_L1\n"
     ]
    }
   ],
   "source": [
    "trainer.save_model(ADAPTER_PATH)\n",
    "tokenizer.save_pretrained(ADAPTER_PATH)\n",
    "print(f\"✅ Adapter saved to {ADAPTER_PATH}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "eddd0d64",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PeftModelForCausalLM(\n",
       "  (base_model): LoraModel(\n",
       "    (model): Qwen2ForCausalLM(\n",
       "      (model): Qwen2Model(\n",
       "        (embed_tokens): Embedding(152064, 3584)\n",
       "        (layers): ModuleList(\n",
       "          (0-27): 28 x Qwen2DecoderLayer(\n",
       "            (self_attn): Qwen2Attention(\n",
       "              (q_proj): lora.Linear4bit(\n",
       "                (base_layer): Linear4bit(in_features=3584, out_features=3584, bias=True)\n",
       "                (lora_dropout): ModuleDict(\n",
       "                  (default): Identity()\n",
       "                )\n",
       "                (lora_A): ModuleDict(\n",
       "                  (default): Linear(in_features=3584, out_features=64, bias=False)\n",
       "                )\n",
       "                (lora_B): ModuleDict(\n",
       "                  (default): Linear(in_features=64, out_features=3584, bias=False)\n",
       "                )\n",
       "                (lora_embedding_A): ParameterDict()\n",
       "                (lora_embedding_B): ParameterDict()\n",
       "                (lora_magnitude_vector): ModuleDict()\n",
       "              )\n",
       "              (k_proj): lora.Linear4bit(\n",
       "                (base_layer): Linear4bit(in_features=3584, out_features=512, bias=True)\n",
       "                (lora_dropout): ModuleDict(\n",
       "                  (default): Identity()\n",
       "                )\n",
       "                (lora_A): ModuleDict(\n",
       "                  (default): Linear(in_features=3584, out_features=64, bias=False)\n",
       "                )\n",
       "                (lora_B): ModuleDict(\n",
       "                  (default): Linear(in_features=64, out_features=512, bias=False)\n",
       "                )\n",
       "                (lora_embedding_A): ParameterDict()\n",
       "                (lora_embedding_B): ParameterDict()\n",
       "                (lora_magnitude_vector): ModuleDict()\n",
       "              )\n",
       "              (v_proj): lora.Linear4bit(\n",
       "                (base_layer): Linear4bit(in_features=3584, out_features=512, bias=True)\n",
       "                (lora_dropout): ModuleDict(\n",
       "                  (default): Identity()\n",
       "                )\n",
       "                (lora_A): ModuleDict(\n",
       "                  (default): Linear(in_features=3584, out_features=64, bias=False)\n",
       "                )\n",
       "                (lora_B): ModuleDict(\n",
       "                  (default): Linear(in_features=64, out_features=512, bias=False)\n",
       "                )\n",
       "                (lora_embedding_A): ParameterDict()\n",
       "                (lora_embedding_B): ParameterDict()\n",
       "                (lora_magnitude_vector): ModuleDict()\n",
       "              )\n",
       "              (o_proj): lora.Linear4bit(\n",
       "                (base_layer): Linear4bit(in_features=3584, out_features=3584, bias=False)\n",
       "                (lora_dropout): ModuleDict(\n",
       "                  (default): Identity()\n",
       "                )\n",
       "                (lora_A): ModuleDict(\n",
       "                  (default): Linear(in_features=3584, out_features=64, bias=False)\n",
       "                )\n",
       "                (lora_B): ModuleDict(\n",
       "                  (default): Linear(in_features=64, out_features=3584, bias=False)\n",
       "                )\n",
       "                (lora_embedding_A): ParameterDict()\n",
       "                (lora_embedding_B): ParameterDict()\n",
       "                (lora_magnitude_vector): ModuleDict()\n",
       "              )\n",
       "            )\n",
       "            (mlp): Qwen2MLP(\n",
       "              (gate_proj): lora.Linear4bit(\n",
       "                (base_layer): Linear4bit(in_features=3584, out_features=18944, bias=False)\n",
       "                (lora_dropout): ModuleDict(\n",
       "                  (default): Identity()\n",
       "                )\n",
       "                (lora_A): ModuleDict(\n",
       "                  (default): Linear(in_features=3584, out_features=64, bias=False)\n",
       "                )\n",
       "                (lora_B): ModuleDict(\n",
       "                  (default): Linear(in_features=64, out_features=18944, bias=False)\n",
       "                )\n",
       "                (lora_embedding_A): ParameterDict()\n",
       "                (lora_embedding_B): ParameterDict()\n",
       "                (lora_magnitude_vector): ModuleDict()\n",
       "              )\n",
       "              (up_proj): lora.Linear4bit(\n",
       "                (base_layer): Linear4bit(in_features=3584, out_features=18944, bias=False)\n",
       "                (lora_dropout): ModuleDict(\n",
       "                  (default): Identity()\n",
       "                )\n",
       "                (lora_A): ModuleDict(\n",
       "                  (default): Linear(in_features=3584, out_features=64, bias=False)\n",
       "                )\n",
       "                (lora_B): ModuleDict(\n",
       "                  (default): Linear(in_features=64, out_features=18944, bias=False)\n",
       "                )\n",
       "                (lora_embedding_A): ParameterDict()\n",
       "                (lora_embedding_B): ParameterDict()\n",
       "                (lora_magnitude_vector): ModuleDict()\n",
       "              )\n",
       "              (down_proj): lora.Linear4bit(\n",
       "                (base_layer): Linear4bit(in_features=18944, out_features=3584, bias=False)\n",
       "                (lora_dropout): ModuleDict(\n",
       "                  (default): Identity()\n",
       "                )\n",
       "                (lora_A): ModuleDict(\n",
       "                  (default): Linear(in_features=18944, out_features=64, bias=False)\n",
       "                )\n",
       "                (lora_B): ModuleDict(\n",
       "                  (default): Linear(in_features=64, out_features=3584, bias=False)\n",
       "                )\n",
       "                (lora_embedding_A): ParameterDict()\n",
       "                (lora_embedding_B): ParameterDict()\n",
       "                (lora_magnitude_vector): ModuleDict()\n",
       "              )\n",
       "              (act_fn): SiLUActivation()\n",
       "            )\n",
       "            (input_layernorm): Qwen2RMSNorm((3584,), eps=1e-06)\n",
       "            (post_attention_layernorm): Qwen2RMSNorm((3584,), eps=1e-06)\n",
       "          )\n",
       "        )\n",
       "        (norm): Qwen2RMSNorm((3584,), eps=1e-06)\n",
       "        (rotary_emb): Qwen2RotaryEmbedding()\n",
       "      )\n",
       "      (lm_head): Linear(in_features=3584, out_features=152064, bias=False)\n",
       "    )\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import TextStreamer\n",
    "\n",
    "# 2. Prepare Model for Inference\n",
    "model.eval()\n",
    "# ❌ REMOVED: model.to(torch.bfloat16) \n",
    "# Why? The model is already loaded in 4-bit with bfloat16 compute type via your bnb_config. \n",
    "# Manually moving it again can break the quantization hooks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "011fcfb5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PeftModelForCausalLM(\n",
       "  (base_model): LoraModel(\n",
       "    (model): Qwen2ForCausalLM(\n",
       "      (model): Qwen2Model(\n",
       "        (embed_tokens): Embedding(152064, 3584)\n",
       "        (layers): ModuleList(\n",
       "          (0-27): 28 x Qwen2DecoderLayer(\n",
       "            (self_attn): Qwen2Attention(\n",
       "              (q_proj): lora.Linear4bit(\n",
       "                (base_layer): Linear4bit(in_features=3584, out_features=3584, bias=True)\n",
       "                (lora_dropout): ModuleDict(\n",
       "                  (default): Identity()\n",
       "                )\n",
       "                (lora_A): ModuleDict(\n",
       "                  (default): Linear(in_features=3584, out_features=64, bias=False)\n",
       "                )\n",
       "                (lora_B): ModuleDict(\n",
       "                  (default): Linear(in_features=64, out_features=3584, bias=False)\n",
       "                )\n",
       "                (lora_embedding_A): ParameterDict()\n",
       "                (lora_embedding_B): ParameterDict()\n",
       "                (lora_magnitude_vector): ModuleDict()\n",
       "              )\n",
       "              (k_proj): lora.Linear4bit(\n",
       "                (base_layer): Linear4bit(in_features=3584, out_features=512, bias=True)\n",
       "                (lora_dropout): ModuleDict(\n",
       "                  (default): Identity()\n",
       "                )\n",
       "                (lora_A): ModuleDict(\n",
       "                  (default): Linear(in_features=3584, out_features=64, bias=False)\n",
       "                )\n",
       "                (lora_B): ModuleDict(\n",
       "                  (default): Linear(in_features=64, out_features=512, bias=False)\n",
       "                )\n",
       "                (lora_embedding_A): ParameterDict()\n",
       "                (lora_embedding_B): ParameterDict()\n",
       "                (lora_magnitude_vector): ModuleDict()\n",
       "              )\n",
       "              (v_proj): lora.Linear4bit(\n",
       "                (base_layer): Linear4bit(in_features=3584, out_features=512, bias=True)\n",
       "                (lora_dropout): ModuleDict(\n",
       "                  (default): Identity()\n",
       "                )\n",
       "                (lora_A): ModuleDict(\n",
       "                  (default): Linear(in_features=3584, out_features=64, bias=False)\n",
       "                )\n",
       "                (lora_B): ModuleDict(\n",
       "                  (default): Linear(in_features=64, out_features=512, bias=False)\n",
       "                )\n",
       "                (lora_embedding_A): ParameterDict()\n",
       "                (lora_embedding_B): ParameterDict()\n",
       "                (lora_magnitude_vector): ModuleDict()\n",
       "              )\n",
       "              (o_proj): lora.Linear4bit(\n",
       "                (base_layer): Linear4bit(in_features=3584, out_features=3584, bias=False)\n",
       "                (lora_dropout): ModuleDict(\n",
       "                  (default): Identity()\n",
       "                )\n",
       "                (lora_A): ModuleDict(\n",
       "                  (default): Linear(in_features=3584, out_features=64, bias=False)\n",
       "                )\n",
       "                (lora_B): ModuleDict(\n",
       "                  (default): Linear(in_features=64, out_features=3584, bias=False)\n",
       "                )\n",
       "                (lora_embedding_A): ParameterDict()\n",
       "                (lora_embedding_B): ParameterDict()\n",
       "                (lora_magnitude_vector): ModuleDict()\n",
       "              )\n",
       "            )\n",
       "            (mlp): Qwen2MLP(\n",
       "              (gate_proj): lora.Linear4bit(\n",
       "                (base_layer): Linear4bit(in_features=3584, out_features=18944, bias=False)\n",
       "                (lora_dropout): ModuleDict(\n",
       "                  (default): Identity()\n",
       "                )\n",
       "                (lora_A): ModuleDict(\n",
       "                  (default): Linear(in_features=3584, out_features=64, bias=False)\n",
       "                )\n",
       "                (lora_B): ModuleDict(\n",
       "                  (default): Linear(in_features=64, out_features=18944, bias=False)\n",
       "                )\n",
       "                (lora_embedding_A): ParameterDict()\n",
       "                (lora_embedding_B): ParameterDict()\n",
       "                (lora_magnitude_vector): ModuleDict()\n",
       "              )\n",
       "              (up_proj): lora.Linear4bit(\n",
       "                (base_layer): Linear4bit(in_features=3584, out_features=18944, bias=False)\n",
       "                (lora_dropout): ModuleDict(\n",
       "                  (default): Identity()\n",
       "                )\n",
       "                (lora_A): ModuleDict(\n",
       "                  (default): Linear(in_features=3584, out_features=64, bias=False)\n",
       "                )\n",
       "                (lora_B): ModuleDict(\n",
       "                  (default): Linear(in_features=64, out_features=18944, bias=False)\n",
       "                )\n",
       "                (lora_embedding_A): ParameterDict()\n",
       "                (lora_embedding_B): ParameterDict()\n",
       "                (lora_magnitude_vector): ModuleDict()\n",
       "              )\n",
       "              (down_proj): lora.Linear4bit(\n",
       "                (base_layer): Linear4bit(in_features=18944, out_features=3584, bias=False)\n",
       "                (lora_dropout): ModuleDict(\n",
       "                  (default): Identity()\n",
       "                )\n",
       "                (lora_A): ModuleDict(\n",
       "                  (default): Linear(in_features=18944, out_features=64, bias=False)\n",
       "                )\n",
       "                (lora_B): ModuleDict(\n",
       "                  (default): Linear(in_features=64, out_features=3584, bias=False)\n",
       "                )\n",
       "                (lora_embedding_A): ParameterDict()\n",
       "                (lora_embedding_B): ParameterDict()\n",
       "                (lora_magnitude_vector): ModuleDict()\n",
       "              )\n",
       "              (act_fn): SiLUActivation()\n",
       "            )\n",
       "            (input_layernorm): Qwen2RMSNorm((3584,), eps=1e-06)\n",
       "            (post_attention_layernorm): Qwen2RMSNorm((3584,), eps=1e-06)\n",
       "          )\n",
       "        )\n",
       "        (norm): Qwen2RMSNorm((3584,), eps=1e-06)\n",
       "        (rotary_emb): Qwen2RotaryEmbedding()\n",
       "      )\n",
       "      (lm_head): Linear(in_features=3584, out_features=152064, bias=False)\n",
       "    )\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.to(torch.bfloat16) # Ensures all weights are viewed through the BF16 lens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "67c21307",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_intent(user_prompt):\n",
    "    messages = [\n",
    "        {\"role\": \"user\", \"content\": user_prompt}\n",
    "    ]\n",
    "    \n",
    "    # Apply Chat Template\n",
    "    inputs = tokenizer.apply_chat_template(\n",
    "        messages, \n",
    "        tokenize=True, \n",
    "        add_generation_prompt=True, \n",
    "        return_tensors=\"pt\"\n",
    "    ).to(\"cuda\")\n",
    "\n",
    "    # ❌ REMOVED: inputs = inputs.to(dtype=torch.bfloat16)\n",
    "    # Why? 'inputs' are indices (Integers). Converting them to Float/BFloat breaks the Embedding Layer.\n",
    "\n",
    "    with torch.no_grad():\n",
    "        outputs = model.generate(\n",
    "            input_ids=inputs, \n",
    "            max_new_tokens=512, \n",
    "            temperature=0.1, \n",
    "            do_sample=True, \n",
    "            top_p=0.95,\n",
    "            pad_token_id=tokenizer.eos_token_id\n",
    "        )\n",
    "\n",
    "    # Decode and Slice\n",
    "    decoded_output = tokenizer.decode(outputs[0][len(inputs[0]):], skip_special_tokens=True)\n",
    "    return decoded_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "b13f79f1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- TEST 1: Standard Portfolio (Should Accept) ---\n",
      "Input: Create a dark mode portfolio for a wedding photographer with gallery and contact.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\prasasnna\\Miniconda3\\envs\\blackwell\\lib\\site-packages\\bitsandbytes\\backends\\cuda\\ops.py:468: FutureWarning: _check_is_size will be removed in a future PyTorch release along with guard_size_oblivious.     Use _check(i >= 0) instead.\n",
      "  torch._check_is_size(blocksize)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output:\n",
      "{\"project_type\":\"blog_page\",\"theme\":\"dark_mode\",\"domain\":\"art_shop\",\"tone\":\"modern\",\"audience\":\"rec_ent\",\"explicit_sections\":[],\"error\":null}\n",
      "\n",
      "--- TEST 2: Messy Input (Should Fix & Normalize) ---\n",
      "Input: i want a vibrant site for my bubble tea shop in Kovilpatti... playful tone.. need menu and location\n",
      "Output:\n",
      "{\"project_type\":\"landing\",\"theme\":\"minimal\",\"domain\":\"professionalaiastsersech\",\"audience\":\"clientsexplicit\",\"explicit_sections\":[],\"error\":null}\n",
      "\n",
      "--- TEST 3: Scope Violation (Should Reject) ---\n",
      "Input: Build me a fully functional e-commerce store with user login and a payment gateway using Python.\n",
      "Output:\n",
      "{\"project_type\": \"landing_page\", \"theme\": \"light_mode\", \"domain\": \"social\", \"tone\": \"modern\", \"audience\": \"general\", \"explicit_sections\": [], \"error\": \"scope_violation\"}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"--- TEST 1: Standard Portfolio (Should Accept) ---\")\n",
    "prompt_1 = \"Create a dark mode portfolio for a wedding photographer with gallery and contact.\"\n",
    "print(f\"Input: {prompt_1}\")\n",
    "print(f\"Output:\\n{generate_intent(prompt_1)}\\n\")\n",
    "\n",
    "print(\"--- TEST 2: Messy Input (Should Fix & Normalize) ---\")\n",
    "prompt_2 = \"i want a vibrant site for my bubble tea shop in Kovilpatti... playful tone.. need menu and location\"\n",
    "print(f\"Input: {prompt_2}\")\n",
    "print(f\"Output:\\n{generate_intent(prompt_2)}\\n\")\n",
    "\n",
    "print(\"--- TEST 3: Scope Violation (Should Reject) ---\")\n",
    "prompt_3 = \"Build me a fully functional e-commerce store with user login and a payment gateway using Python.\"\n",
    "print(f\"Input: {prompt_3}\")\n",
    "print(f\"Output:\\n{generate_intent(prompt_3)}\\n\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "blackwell",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
