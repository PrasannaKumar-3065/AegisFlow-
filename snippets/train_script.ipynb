{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9415649d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\prasasnna\\Miniconda3\\envs\\blackwell\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "Skipping import of cpp extensions due to incompatible torch version 2.12.0.dev20260221+cu128 for torchao version 0.16.0             Please see https://github.com/pytorch/ao/issues/2919 for more info\n",
      "W0222 20:37:51.292000 29776 site-packages\\torch\\distributed\\elastic\\multiprocessing\\redirects.py:29] NOTE: Redirects are currently not supported in Windows or MacOs.\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import os\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer, BitsAndBytesConfig\n",
    "from peft import LoraConfig, get_peft_model\n",
    "from trl import SFTTrainer, SFTConfig\n",
    "from datasets import load_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "187bf2ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "BASE_DIR = os.getcwd()\n",
    "INPUT_PATH   = os.path.join(BASE_DIR, \"..\", \"datasets\", \"L3\", \"L3_dataser_v1.jsonl\")\n",
    "ADAPTER_PATH = os.path.join(BASE_DIR, \"..\", \"adapters\", \"L3\",\"aegis_L3_v1\")\n",
    "model_name = \"Qwen/Qwen2.5-Coder-1.5B-Instruct\"\n",
    "CURRENT_LAYER = 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "bf844bae",
   "metadata": {},
   "outputs": [],
   "source": [
    "bnb_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,\n",
    "    bnb_4bit_use_double_quant=True,\n",
    "    bnb_4bit_quant_type=\"nf4\",\n",
    "    bnb_4bit_compute_dtype=torch.bfloat16, # RTX 50-series supports native BF16\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "dfd05edd",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "tokenizer.padding_side = \"right\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "dc0c8e7a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`torch_dtype` is deprecated! Use `dtype` instead!\n",
      "c:\\Users\\prasasnna\\Miniconda3\\envs\\blackwell\\lib\\site-packages\\bitsandbytes\\backends\\cuda\\ops.py:213: FutureWarning: _check_is_size will be removed in a future PyTorch release along with guard_size_oblivious.     Use _check(i >= 0) instead.\n",
      "  torch._check_is_size(blocksize)\n"
     ]
    }
   ],
   "source": [
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_name,\n",
    "    quantization_config=bnb_config,\n",
    "    torch_dtype=torch.bfloat16,\n",
    "    device_map=\"auto\",\n",
    "    attn_implementation=\"sdpa\", # Standard SDPA for stability\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e1d8aa14",
   "metadata": {},
   "outputs": [],
   "source": [
    "lora_config = LoraConfig(\n",
    "    r=128, \n",
    "    lora_alpha=256,\n",
    "    target_modules=[\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\", \"gate_proj\", \"up_proj\", \"down_proj\"],\n",
    "    lora_dropout=0.05,\n",
    "    bias=\"none\",\n",
    "    task_type=\"CAUSAL_LM\",\n",
    ")\n",
    "model = get_peft_model(model, lora_config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "cbab251f",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = load_dataset(\"json\", data_files=INPUT_PATH, split=\"train\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "0ad45641",
   "metadata": {},
   "outputs": [],
   "source": [
    "def formatting_func(example):\n",
    "    # This uses the system prompt you already have in the file\n",
    "    text = tokenizer.apply_chat_template(example[\"messages\"], tokenize=False, add_generation_prompt=False)\n",
    "    return {\"text\": text}\n",
    "\n",
    "dataset = dataset.map(formatting_func)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "a84b15f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "sft_config = SFTConfig(\n",
    "    output_dir=\"outputs\",\n",
    "    dataset_text_field=\"text\",\n",
    "    per_device_train_batch_size=2,   # Small batch for 12GB\n",
    "    gradient_accumulation_steps=8,  # High accumulation to keep effective batch size at 16\n",
    "    num_train_epochs=3,             # Higher epochs to solidify the strict JSON rules\n",
    "    learning_rate=1e-4,             # Lower LR for better convergence on strict enums\n",
    "    bf16=True,\n",
    "    optim=\"paged_adamw_8bit\",       # Paged optimizer prevents OOM spikes\n",
    "    logging_steps=1,\n",
    "    gradient_checkpointing=True,    # Crucial for 12GB VRAM\n",
    "    save_strategy=\"epoch\",\n",
    "    max_length=2048,\n",
    "    weight_decay=0.1\n",
    ")\n",
    "\n",
    "trainer = SFTTrainer(\n",
    "    model=model,\n",
    "    train_dataset=dataset,\n",
    "    args=sft_config,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f5c4fe4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The tokenizer has new PAD/BOS/EOS tokens that differ from the model config and generation config. The model config and generation config were aligned accordingly, being updated with the tokenizer's values. Updated tokens: {'bos_token_id': None, 'pad_token_id': 151643}.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='64' max='189' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [ 64/189 46:20 < 1:33:26, 0.02 it/s, Epoch 1/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>1.309800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>1.029300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.824300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0.658000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>0.560200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>0.424000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>0.343300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>0.274600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9</td>\n",
       "      <td>0.237700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>0.245700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11</td>\n",
       "      <td>0.191000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12</td>\n",
       "      <td>0.190100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>13</td>\n",
       "      <td>0.176300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>14</td>\n",
       "      <td>0.155700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>15</td>\n",
       "      <td>0.149000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>16</td>\n",
       "      <td>0.141800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>17</td>\n",
       "      <td>0.151600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>18</td>\n",
       "      <td>0.119600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>19</td>\n",
       "      <td>0.136500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>20</td>\n",
       "      <td>0.123600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>21</td>\n",
       "      <td>0.121900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>22</td>\n",
       "      <td>0.117400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>23</td>\n",
       "      <td>0.106600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>24</td>\n",
       "      <td>0.083800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>25</td>\n",
       "      <td>0.084700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>26</td>\n",
       "      <td>0.087200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>27</td>\n",
       "      <td>0.087500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>28</td>\n",
       "      <td>0.057500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>29</td>\n",
       "      <td>0.057100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>30</td>\n",
       "      <td>0.066100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>31</td>\n",
       "      <td>0.046800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>32</td>\n",
       "      <td>0.047800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>33</td>\n",
       "      <td>0.042100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>34</td>\n",
       "      <td>0.041300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>35</td>\n",
       "      <td>0.044700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>36</td>\n",
       "      <td>0.044000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>37</td>\n",
       "      <td>0.027500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>38</td>\n",
       "      <td>0.027400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>39</td>\n",
       "      <td>0.023300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>40</td>\n",
       "      <td>0.022300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>41</td>\n",
       "      <td>0.018100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>42</td>\n",
       "      <td>0.017200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>43</td>\n",
       "      <td>0.017200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>44</td>\n",
       "      <td>0.023300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>45</td>\n",
       "      <td>0.019600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>46</td>\n",
       "      <td>0.013900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>47</td>\n",
       "      <td>0.012800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>48</td>\n",
       "      <td>0.009200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>49</td>\n",
       "      <td>0.010300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>50</td>\n",
       "      <td>0.011600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>51</td>\n",
       "      <td>0.010900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>52</td>\n",
       "      <td>0.007900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>53</td>\n",
       "      <td>0.007400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>54</td>\n",
       "      <td>0.009100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>55</td>\n",
       "      <td>0.007400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>56</td>\n",
       "      <td>0.007500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>57</td>\n",
       "      <td>0.006500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>58</td>\n",
       "      <td>0.006900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>59</td>\n",
       "      <td>0.006700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>60</td>\n",
       "      <td>0.006900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>61</td>\n",
       "      <td>0.006100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>62</td>\n",
       "      <td>0.005800</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "b9436c9c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Training Complete. Adapter saved to d:\\Python\\AegisFlow-\\snippets\\..\\adapters\\aegis_L2_v1\n"
     ]
    }
   ],
   "source": [
    "trainer.save_model(ADAPTER_PATH)\n",
    "tokenizer.save_pretrained(ADAPTER_PATH)\n",
    "print(f\"âœ… Training Complete. Adapter saved to {ADAPTER_PATH}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "2dd6f28d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PeftModelForCausalLM(\n",
       "  (base_model): LoraModel(\n",
       "    (model): Qwen2ForCausalLM(\n",
       "      (model): Qwen2Model(\n",
       "        (embed_tokens): Embedding(151936, 1536)\n",
       "        (layers): ModuleList(\n",
       "          (0-27): 28 x Qwen2DecoderLayer(\n",
       "            (self_attn): Qwen2Attention(\n",
       "              (q_proj): lora.Linear4bit(\n",
       "                (base_layer): Linear4bit(in_features=1536, out_features=1536, bias=True)\n",
       "                (lora_dropout): ModuleDict(\n",
       "                  (default): Dropout(p=0.05, inplace=False)\n",
       "                )\n",
       "                (lora_A): ModuleDict(\n",
       "                  (default): Linear(in_features=1536, out_features=128, bias=False)\n",
       "                )\n",
       "                (lora_B): ModuleDict(\n",
       "                  (default): Linear(in_features=128, out_features=1536, bias=False)\n",
       "                )\n",
       "                (lora_embedding_A): ParameterDict()\n",
       "                (lora_embedding_B): ParameterDict()\n",
       "                (lora_magnitude_vector): ModuleDict()\n",
       "              )\n",
       "              (k_proj): lora.Linear4bit(\n",
       "                (base_layer): Linear4bit(in_features=1536, out_features=256, bias=True)\n",
       "                (lora_dropout): ModuleDict(\n",
       "                  (default): Dropout(p=0.05, inplace=False)\n",
       "                )\n",
       "                (lora_A): ModuleDict(\n",
       "                  (default): Linear(in_features=1536, out_features=128, bias=False)\n",
       "                )\n",
       "                (lora_B): ModuleDict(\n",
       "                  (default): Linear(in_features=128, out_features=256, bias=False)\n",
       "                )\n",
       "                (lora_embedding_A): ParameterDict()\n",
       "                (lora_embedding_B): ParameterDict()\n",
       "                (lora_magnitude_vector): ModuleDict()\n",
       "              )\n",
       "              (v_proj): lora.Linear4bit(\n",
       "                (base_layer): Linear4bit(in_features=1536, out_features=256, bias=True)\n",
       "                (lora_dropout): ModuleDict(\n",
       "                  (default): Dropout(p=0.05, inplace=False)\n",
       "                )\n",
       "                (lora_A): ModuleDict(\n",
       "                  (default): Linear(in_features=1536, out_features=128, bias=False)\n",
       "                )\n",
       "                (lora_B): ModuleDict(\n",
       "                  (default): Linear(in_features=128, out_features=256, bias=False)\n",
       "                )\n",
       "                (lora_embedding_A): ParameterDict()\n",
       "                (lora_embedding_B): ParameterDict()\n",
       "                (lora_magnitude_vector): ModuleDict()\n",
       "              )\n",
       "              (o_proj): lora.Linear4bit(\n",
       "                (base_layer): Linear4bit(in_features=1536, out_features=1536, bias=False)\n",
       "                (lora_dropout): ModuleDict(\n",
       "                  (default): Dropout(p=0.05, inplace=False)\n",
       "                )\n",
       "                (lora_A): ModuleDict(\n",
       "                  (default): Linear(in_features=1536, out_features=128, bias=False)\n",
       "                )\n",
       "                (lora_B): ModuleDict(\n",
       "                  (default): Linear(in_features=128, out_features=1536, bias=False)\n",
       "                )\n",
       "                (lora_embedding_A): ParameterDict()\n",
       "                (lora_embedding_B): ParameterDict()\n",
       "                (lora_magnitude_vector): ModuleDict()\n",
       "              )\n",
       "            )\n",
       "            (mlp): Qwen2MLP(\n",
       "              (gate_proj): lora.Linear4bit(\n",
       "                (base_layer): Linear4bit(in_features=1536, out_features=8960, bias=False)\n",
       "                (lora_dropout): ModuleDict(\n",
       "                  (default): Dropout(p=0.05, inplace=False)\n",
       "                )\n",
       "                (lora_A): ModuleDict(\n",
       "                  (default): Linear(in_features=1536, out_features=128, bias=False)\n",
       "                )\n",
       "                (lora_B): ModuleDict(\n",
       "                  (default): Linear(in_features=128, out_features=8960, bias=False)\n",
       "                )\n",
       "                (lora_embedding_A): ParameterDict()\n",
       "                (lora_embedding_B): ParameterDict()\n",
       "                (lora_magnitude_vector): ModuleDict()\n",
       "              )\n",
       "              (up_proj): lora.Linear4bit(\n",
       "                (base_layer): Linear4bit(in_features=1536, out_features=8960, bias=False)\n",
       "                (lora_dropout): ModuleDict(\n",
       "                  (default): Dropout(p=0.05, inplace=False)\n",
       "                )\n",
       "                (lora_A): ModuleDict(\n",
       "                  (default): Linear(in_features=1536, out_features=128, bias=False)\n",
       "                )\n",
       "                (lora_B): ModuleDict(\n",
       "                  (default): Linear(in_features=128, out_features=8960, bias=False)\n",
       "                )\n",
       "                (lora_embedding_A): ParameterDict()\n",
       "                (lora_embedding_B): ParameterDict()\n",
       "                (lora_magnitude_vector): ModuleDict()\n",
       "              )\n",
       "              (down_proj): lora.Linear4bit(\n",
       "                (base_layer): Linear4bit(in_features=8960, out_features=1536, bias=False)\n",
       "                (lora_dropout): ModuleDict(\n",
       "                  (default): Dropout(p=0.05, inplace=False)\n",
       "                )\n",
       "                (lora_A): ModuleDict(\n",
       "                  (default): Linear(in_features=8960, out_features=128, bias=False)\n",
       "                )\n",
       "                (lora_B): ModuleDict(\n",
       "                  (default): Linear(in_features=128, out_features=1536, bias=False)\n",
       "                )\n",
       "                (lora_embedding_A): ParameterDict()\n",
       "                (lora_embedding_B): ParameterDict()\n",
       "                (lora_magnitude_vector): ModuleDict()\n",
       "              )\n",
       "              (act_fn): SiLUActivation()\n",
       "            )\n",
       "            (input_layernorm): Qwen2RMSNorm((1536,), eps=1e-06)\n",
       "            (post_attention_layernorm): Qwen2RMSNorm((1536,), eps=1e-06)\n",
       "          )\n",
       "        )\n",
       "        (norm): Qwen2RMSNorm((1536,), eps=1e-06)\n",
       "        (rotary_emb): Qwen2RotaryEmbedding()\n",
       "      )\n",
       "      (lm_head): Linear(in_features=1536, out_features=151936, bias=False)\n",
       "    )\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4efa88a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_intent_L1(user_prompt):\n",
    "    # This must match the system prompt in your L1_dataset_v3.jsonl exactly\n",
    "    system_prompt = (\n",
    "        \"You are L1 of Mini Replit. Extract intent from user prompts as strict JSON only. \"\n",
    "        \"No explanation. No markdown.\\n\"\n",
    "        \"Enums â€” project_type: landing_page|portfolio|blog  theme: dark_mode|light|minimal|vibrant  \"\n",
    "        \"tone: modern|professional|playful|bold\\n\"\n",
    "        \"domain: snake_case string. audience: target audience string.\\n\"\n",
    "        \"explicit_sections: normalize user terms to: navbar,hero,features,about,services,pricing,\"\n",
    "        \"testimonials,gallery,faq,blog,contact,call_to_action,footer,section_generic \"\n",
    "        \"(menuâ†’navbar, reviewsâ†’testimonials, about meâ†’about, locationâ†’contact). Deduplicate.\\n\"\n",
    "        \"error: scope_violation if request needs backend/auth/payments/realtime/DB. Else null.\\n\"\n",
    "        \"On scope_violation: still fill all fields with best-effort values.\\n\"\n",
    "        \"Schema: {\\\"project_type\\\":\\\"...\\\",\\\"theme\\\":\\\"...\\\",\\\"domain\\\":\\\"...\\\",\\\"tone\\\":\\\"...\\\",\\\"audience\\\":\\\"...\\\",\\\"explicit_sections\\\":[...],\\\"error\\\":null}\"\n",
    "    )\n",
    "\n",
    "    messages = [\n",
    "        {\"role\": \"system\", \"content\": system_prompt},\n",
    "        {\"role\": \"user\", \"content\": user_prompt}\n",
    "    ]\n",
    "    \n",
    "    # Apply the chat template\n",
    "    # add_generation_prompt=True ensures the model starts exactly at the assistant's JSON response\n",
    "    inputs = tokenizer.apply_chat_template(\n",
    "        messages, \n",
    "        tokenize=True, \n",
    "        add_generation_prompt=True, \n",
    "        return_tensors=\"pt\"\n",
    "    ).to(\"cuda\")\n",
    "\n",
    "    # Generate with high determinism\n",
    "    with torch.no_grad():\n",
    "        outputs = model.generate(\n",
    "            input_ids=inputs,\n",
    "            max_new_tokens=256, \n",
    "            temperature=0,      # Zero temperature for deterministic output\n",
    "            do_sample=False,    # Disable sampling to prevent hallucinations like \"dog_deography\"\n",
    "            pad_token_id=tokenizer.eos_token_id,\n",
    "            eos_token_id=tokenizer.eos_token_id\n",
    "        )\n",
    "\n",
    "    # Decode and remove the prompt tokens\n",
    "    decoded_output = tokenizer.decode(outputs[0][len(inputs[0]):], skip_special_tokens=True)\n",
    "    return decoded_output.strip()\n",
    "\n",
    "def generate_structure_L2(l1_intent_json):\n",
    "    \"\"\"\n",
    "    Takes the JSON output from L1 and generates the L2 Structural Skeleton.\n",
    "    Ensures attention_mask is passed to avoid 'unexpected behavior'.\n",
    "    \"\"\"\n",
    "    # MUST match L2_dataset_v2.jsonl system prompt EXACTLY\n",
    "    system_prompt = (\n",
    "        \"You are L2 of Mini Replit. Input: intent JSON. Output: ONLY strict JSON with exactly 3 keys: \"\n",
    "        \"pages, constraints, file_tree. NEVER copy input fields into output. No explanation. No markdown.\\n\\n\"\n",
    "        \"Schema: {\\\"pages\\\":{\\\"index.html\\\":{\\\"sections\\\":[{\\\"id\\\":\\\"<n>\\\",\\\"tag\\\":\\\"<t>\\\",\\\"class\\\":\\\"<n>\\\",\\\"layout\\\":\\\"<l>\\\"},...]}},\"\n",
    "        \"\\\"constraints\\\":[\\\"semantic_html\\\",\\\"responsive\\\",\\\"external_css_only\\\",\\\"no_inline_styles\\\",\\\"no_script_tags\\\"],\"\n",
    "        \"\\\"file_tree\\\":[\\\"index.html\\\",\\\"styles.css\\\"]}\\n\\n\"\n",
    "        \"Tag rules (exact):\\n  navbarâ†’header  footerâ†’footer  EVERYTHING ELSEâ†’section\\n\"\n",
    "        \"  (tag is NEVER 'grid' or 'block' â€” those are layouts, not tags)\\n\\n\"\n",
    "        \"Layout rules:\\n  flex:  navbar, hero, contact, call_to_action, footer\\n\"\n",
    "        \"  grid:  features, pricing, testimonials, gallery\\n\"\n",
    "        \"  block: about, services, faq, blog, section_generic\\n\\n\"\n",
    "        \"Section order (always):\\n  navbar(1st) â†’ hero(2nd,MANDATORY) â†’ core sections â†’ call_to_action â†’ contact â†’ footer(LAST,MANDATORY)\\n\\n\"\n",
    "        \"Core section canonical order: featuresâ†’aboutâ†’servicesâ†’pricingâ†’testimonialsâ†’galleryâ†’faqâ†’blog\\n\"\n",
    "        \"Max 4 core sections. If input has more, keep first 4 by canonical order.\\n\\n\"\n",
    "        \"Defaults when core is empty:\\n  landing_pageâ†’features  portfolioâ†’gallery  blogâ†’blog\\n\"\n",
    "        \"portfolio: ALWAYS include gallery even if not in explicit_sections.\\n\\n\"\n",
    "        \"class must ALWAYS equal id. No exceptions.\"\n",
    "    )\n",
    "\n",
    "    messages = [\n",
    "        {\"role\": \"system\", \"content\": system_prompt},\n",
    "        {\"role\": \"user\", \"content\": l1_intent_json}\n",
    "    ]\n",
    "    \n",
    "    # âœ… FIX 1: return_dict=True to get the attention_mask\n",
    "    inputs = tokenizer.apply_chat_template(\n",
    "        messages, \n",
    "        tokenize=True, \n",
    "        add_generation_prompt=True, \n",
    "        return_tensors=\"pt\",\n",
    "        return_dict=True\n",
    "    ).to(\"cuda\")\n",
    "\n",
    "    with torch.no_grad():\n",
    "        outputs = model.generate(\n",
    "            **inputs, # âœ… FIX 2: Pass both input_ids and attention_mask\n",
    "            max_new_tokens=1536,\n",
    "            temperature=0,       \n",
    "            do_sample=False,     \n",
    "            pad_token_id=tokenizer.eos_token_id,\n",
    "            eos_token_id=tokenizer.eos_token_id\n",
    "        )\n",
    "\n",
    "    # Decode using the correct input_ids key\n",
    "    decoded_output = tokenizer.decode(outputs[0][len(inputs['input_ids'][0]):], skip_special_tokens=True)\n",
    "    return decoded_output.strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "f33dd31b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ”¥ STARTING L2 STRESS TEST ðŸ”¥\n",
      "\n",
      "--- STRESS 1: The Kitchen Sink (Limit & Order) ---\n",
      "Input Intent: {\"project_type\":\"landing_page\",\"explicit_sections\":[\"blog\",\"faq\",\"gallery\",\"testimonials\",\"pricing\",\"services\",\"about\",\"features\"]}\n",
      "Output:\n",
      "{\"pages\":{\"index.html\":{\"sections\":[{\"id\":\"navbar\",\"tag\":\"header\",\"class\":\"navbar\",\"layout\":\"flex\"},{\"id\":\"hero\",\"tag\":\"section\",\"class\":\"hero\",\"layout\":\"flex\"},{\"id\":\"features\",\"tag\":\"section\",\"class\":\"features\",\"layout\":\"grid\"},{\"id\":\"pricing\",\"tag\":\"section\",\"class\":\"pricing\",\"layout\":\"grid\"},{\"id\":\"testimonials\",\"tag\":\"section\",\"class\":\"testimonials\",\"layout\":\"grid\"},{\"id\":\"gallery\",\"tag\":\"section\",\"class\":\"gallery\",\"layout\":\"grid\"},{\"id\":\"footer\",\"tag\":\"footer\",\"class\":\"footer\",\"layout\":\"flex\"}]}},\"constraints\":[\"semantic_html\",\"responsive\",\"external_css_only\",\"no_inline_styles\",\"no_script_tags\"],\"file_tree\":[\"index.html\",\"styles.css\"]}\n",
      "\n",
      "--- STRESS 2: The Silent Portfolio (Defaulting) ---\n",
      "Input Intent: {\"project_type\":\"portfolio\",\"explicit_sections\":[]}\n",
      "Output:\n",
      "{\"pages\":{\"index.html\":{\"sections\":[{\"id\":\"navbar\",\"tag\":\"header\",\"class\":\"navbar\",\"layout\":\"flex\"},{\"id\":\"hero\",\"tag\":\"section\",\"class\":\"hero\",\"layout\":\"flex\"},{\"id\":\"gallery\",\"tag\":\"section\",\"class\":\"gallery\",\"layout\":\"grid\"},{\"id\":\"footer\",\"tag\":\"footer\",\"class\":\"footer\",\"layout\":\"flex\"}]}},\"constraints\":[\"semantic_html\",\"responsive\",\"external_css_only\",\"no_inline_styles\",\"no_script_tags\"],\"file_tree\":[\"index.html\",\"styles.css\"]}\n",
      "\n",
      "--- STRESS 3: The Scrambled Egg (Sorting) ---\n",
      "Input Intent: {\"project_type\":\"landing_page\",\"explicit_sections\":[\"contact\",\"pricing\",\"about\",\"navbar\"]}\n",
      "Output:\n",
      "{\"pages\":{\"index.html\":{\"sections\":[{\"id\":\"navbar\",\"tag\":\"header\",\"class\":\"navbar\",\"layout\":\"flex\"},{\"id\":\"hero\",\"tag\":\"section\",\"class\":\"hero\",\"layout\":\"flex\"},{\"id\":\"about\",\"tag\":\"section\",\"class\":\"about\",\"layout\":\"block\"},{\"id\":\"pricing\",\"tag\":\"section\",\"class\":\"pricing\",\"layout\":\"grid\"},{\"id\":\"contact\",\"tag\":\"section\",\"class\":\"contact\",\"layout\":\"flex\"},{\"id\":\"footer\",\"tag\":\"footer\",\"class\":\"footer\",\"layout\":\"flex\"}]}},\"constraints\":[\"semantic_html\",\"responsive\",\"external_css_only\",\"no_inline_styles\",\"no_script_tags\"],\"file_tree\":[\"index.html\",\"styles.css\"]}\n",
      "\n",
      "--- STRESS 4: Minimalist Request (Minimums) ---\n",
      "Input Intent: {\"project_type\":\"landing_page\",\"explicit_sections\":[]}\n",
      "Output:\n",
      "{\"pages\":{\"index.html\":{\"sections\":[{\"id\":\"navbar\",\"tag\":\"header\",\"class\":\"navbar\",\"layout\":\"flex\"},{\"id\":\"hero\",\"tag\":\"section\",\"class\":\"hero\",\"layout\":\"flex\"},{\"id\":\"features\",\"tag\":\"section\",\"class\":\"features\",\"layout\":\"grid\"},{\"id\":\"footer\",\"tag\":\"footer\",\"class\":\"footer\",\"layout\":\"flex\"}]}},\"constraints\":[\"semantic_html\",\"responsive\",\"external_css_only\",\"no_inline_styles\",\"no_script_tags\"],\"file_tree\":[\"index.html\",\"styles.css\"]}\n",
      "\n",
      "--- STRESS 5: The Blog-Only (Defaulting) ---\n",
      "Input Intent: {\"project_type\":\"blog\",\"explicit_sections\":[\"contact\"]}\n",
      "Output:\n",
      "{\"pages\":{\"index.html\":{\"sections\":[{\"id\":\"navbar\",\"tag\":\"header\",\"class\":\"navbar\",\"layout\":\"flex\"},{\"id\":\"hero\",\"tag\":\"section\",\"class\":\"hero\",\"layout\":\"flex\"},{\"id\":\"blog\",\"tag\":\"section\",\"class\":\"blog\",\"layout\":\"block\"},{\"id\":\"contact\",\"tag\":\"section\",\"class\":\"contact\",\"layout\":\"flex\"},{\"id\":\"footer\",\"tag\":\"footer\",\"class\":\"footer\",\"layout\":\"flex\"}]}},\"constraints\":[\"semantic_html\",\"responsive\",\"external_css_only\",\"no_inline_styles\",\"no_script_tags\"],\"file_tree\":[\"index.html\",\"styles.css\"]}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "if CURRENT_LAYER == 1:\n",
    "    print(\"--- TEST 1: The Wedding Photographer (Standard) ---\")\n",
    "    print(generate_intent_L1(\"Create a dark mode portfolio for a wedding photographer with gallery and contact.\"))\n",
    "\n",
    "    print(\"\\n--- TEST 2: The Bubble Tea Shop (Messy/Normalization) ---\")\n",
    "    print(generate_intent_L1(\"vibrant site for bubble tea in Kovilpatti. playful tone. include menu and locations.\"))\n",
    "\n",
    "    print(\"\\n--- TEST 3: The E-commerce Request (Scope Violation) ---\")\n",
    "    print(generate_intent_L1(\"Build an e-commerce store with checkout and user login.\"))\n",
    "\n",
    "    print(\"\\n--- TEST 4: The Empty Prompt (Minimalist) ---\")\n",
    "    print(generate_intent_L1(\"make a site\"))\n",
    "    \n",
    "elif CURRENT_LAYER == 2:\n",
    "   # ==========================================\n",
    "    # L2 FULL STRESS TEST BATTERY\n",
    "    # ==========================================\n",
    "\n",
    "    print(\"ðŸ”¥ STARTING L2 STRESS TEST ðŸ”¥\\n\")\n",
    "\n",
    "    # STRESS 1: The Kitchen Sink (Over-limit & Canonical Order)\n",
    "    # Input provides 8 core sections. L2 must only pick the first 4 based on canonical order.\n",
    "    print(\"--- STRESS 1: The Kitchen Sink (Limit & Order) ---\")\n",
    "    intent_s1 = '{\"project_type\":\"landing_page\",\"explicit_sections\":[\"blog\",\"faq\",\"gallery\",\"testimonials\",\"pricing\",\"services\",\"about\",\"features\"]}'\n",
    "    print(f\"Input Intent: {intent_s1}\")\n",
    "    print(f\"Output:\\n{generate_structure_L2(intent_s1)}\\n\")\n",
    "\n",
    "\n",
    "    # STRESS 2: The Silent Portfolio (Defaulting Logic)\n",
    "    # No sections requested. Model must inject 'gallery' because it's a portfolio.\n",
    "    print(\"--- STRESS 2: The Silent Portfolio (Defaulting) ---\")\n",
    "    intent_s2 = '{\"project_type\":\"portfolio\",\"explicit_sections\":[]}'\n",
    "    print(f\"Input Intent: {intent_s2}\")\n",
    "    print(f\"Output:\\n{generate_structure_L2(intent_s2)}\\n\")\n",
    "\n",
    "\n",
    "    # STRESS 3: The Scrambled Egg (Input Order vs. Canonical Order)\n",
    "    # User asks for sections in the 'wrong' order. L2 must sort them correctly.\n",
    "    print(\"--- STRESS 3: The Scrambled Egg (Sorting) ---\")\n",
    "    intent_s3 = '{\"project_type\":\"landing_page\",\"explicit_sections\":[\"contact\",\"pricing\",\"about\",\"navbar\"]}'\n",
    "    print(f\"Input Intent: {intent_s3}\")\n",
    "    print(f\"Output:\\n{generate_structure_L2(intent_s3)}\\n\")\n",
    "\n",
    "\n",
    "    # STRESS 4: Minimalist Request (Skeleton Minimums)\n",
    "    # Testing if mandatory navbar, hero, and footer are always present even if unrequested.\n",
    "    print(\"--- STRESS 4: Minimalist Request (Minimums) ---\")\n",
    "    intent_s4 = '{\"project_type\":\"landing_page\",\"explicit_sections\":[]}'\n",
    "    print(f\"Input Intent: {intent_s4}\")\n",
    "    print(f\"Output:\\n{generate_structure_L2(intent_s4)}\\n\")\n",
    "\n",
    "\n",
    "    # STRESS 5: The \"Blog-Only\" (Deep Defaults)\n",
    "    print(\"--- STRESS 5: The Blog-Only (Defaulting) ---\")\n",
    "    intent_s5 = '{\"project_type\":\"blog\",\"explicit_sections\":[\"contact\"]}'\n",
    "    print(f\"Input Intent: {intent_s5}\")\n",
    "    print(f\"Output:\\n{generate_structure_L2(intent_s5)}\\n\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "blackwell",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
